# Environment tag
environment: live
tag: latest
tribe: yourtribe
squad: yoursquad

jobs:
  migrate:
    repository: quay.io/hellofresh/<CHARTNAME>
    pullPolicy: Always
    restartPolicy: Never
    command:
      -  "./application"
      - "migrate"
    annotations:
      "helm.sh/hook": post-install, post-upgrade
      "helm.sh/hook-delete-policy": hook-succeeded, hook-failed

deployments:
  api:
    # Number of pods to run
    replicaCount: 2
    # More info: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
    # Must be less than "replicaCount" to make the update rolling-like with some pods always available
    minAvailable: 1
    # Same as above, but controls availability from another side of it
    #maxUnavailable: 1
    containerPort: 80
    repository: quay.io/hellofresh/<CHARTNAME>
    pullPolicy: IfNotPresent
    #env:
    #  foo: "bar"
    # https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/
    #podAnnotations:
    #  custom-annotation: "<CHARTNAME>-api-example"

    # More info
    # https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/
    livenessProbe:
      httpGet:
        # This endpoint should check application LIVENESS. If the endpoint fails several times in a row
        # (see failureThreshold value below) the cluster will TERMINATE the failing pod and create new one.
        # This endpoint should NOT check the state of dependencies, like DB, RabbitMQ, external services, etc.,
        # as restarting the application will not fix failing dependencies.
        # For most web services this endpoint should return some static text, like "Welcome to MyService",
        # just to show that application is able to serve traffic.
        path: /
        port: http
      initialDelaySeconds: 15
      periodSeconds: 10
      timeoutSeconds: 3
      successThreshold: 1
      failureThreshold: 3
    readinessProbe:
      httpGet:
        # This endpoint should check application READINESS. If the endpoint fails several times in a row
        # (see failureThreshold value below) the cluster will remove the pod from upstreams to DISABLE any traffic.
        # Instead of requests failing in your application, callers will get failures on the load balancer level.
        # This prevents overloading downstream dependencies and causing cascading failures.
        # This endpoint SHOULD check the state of dependencies, but only Single Points of Failure
        # (SPoF - dependency that causes application failures in 100% of the requests),
        # like DB, RabbitMQ, external services, etc.
        path: /status
        port: http
      initialDelaySeconds: 15
      periodSeconds: 10
      timeoutSeconds: 3
      successThreshold: 1
      failureThreshold: 3
    resources:
      requests:
        cpu: 250m
        memory: 100Mi

    nodeSelector: {}
    tolerations: []
    affinity: {}
    hpa:
      enabled: false
      #
      # To enable HPA, use values according to below example
      #enabled: true
      #replicas:
      #  min: <Minimum Number of Pods>
      #  max: <Maximum Number of Pods>
      #metrics:
      #- type: Resource
      #  resource:
      #    name: cpu
      #    targetAverageUtilization: <Target CPU utilization across pods>

    phpFPMMetricsExporter:
      enabled: false
      tcpPort: 9253
      repository: hipages/php-fpm_exporter
      tag: "1.0"
      portName: "fpm-metrics"

  consumers:
    # Number of pods to run
    replicaCount: 2
    containerPort: 80
    repository: quay.io/hellofresh/<CHARTNAME>
    pullPolicy: IfNotPresent
    # More info
    # https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/
    livenessProbe:
      # This command should check CLI application liveness.
      # In most cases you just should check if the application can bootstrap.
      exec:
        command:
          -  "./application"
          - "version"
      initialDelaySeconds: 15
      periodSeconds: 10
      timeoutSeconds: 3
      successThreshold: 1
      failureThreshold: 3
    readinessProbe:
      # This command should check CLI application readiness.
      # In most cases you just do not need this - your application should either handle dependencies failures
      # or just fail and cluster will restart it.
      exec:
        command:
          -  "./application"
          - "status"
      initialDelaySeconds: 15
      periodSeconds: 10
      timeoutSeconds: 3
      successThreshold: 1
      failureThreshold: 3
    resources:
      requests:
        cpu: 250m
        memory: 100Mi

    nodeSelector: {}
    tolerations: []
    affinity: {}
    hpa:
      enabled: false
      #
      # To enable HPA, use values according to below example
      #enabled: true
      #replicas:
      #  min: <Minimum Number of Pods>
      #  max: <Maximum Number of Pods>
      #metrics:
      #- type: Resource
      #  resource:
      #    name: cpu
      #    targetAverageUtilization: <Target CPU utilization across pods>

services:
  api:
    enabled: true
    # Enable prometheus metrics scraping
    enablePrometheus: false
    type: ClusterIP
  consumers:
    enabled: false

# Enable if the service is a producer and needs access from legacy infrastructure
ingresses:
  api:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: "nginx"
    path: /
    hosts:
      - <CHARTNAME>.live-k8s.hellofresh.io
  consumers:
    enabled: false

cronJobs:
  cronjob1:
    repository: quay.io/hellofresh/<CHARTNAME>
    pullPolicy: Always
    schedule: "*/20 * * * *"
    command: ["/bin/echo"]
    args:
      - "$(date)"
    failedJobsHistoryLimit: 1
    successfulJobsHistoryLimit: 3
    concurrencyPolicy: Allow
    startingDeadlineSeconds: 100

# KV pairs for values for ENV vars
configMap:
  foo: "bar"

# Prometheus Alerts
# Remove if not needed
prometheusRule:
- name: <CHARTNAME>.rules
  rules:
  - alert: MyServiceHighErrorRate
    annotations:
      description: "Service Down"
      summary: Service is down
    expr: absent(up{job="my-service-k8s"} == 1)
    for: 5m
    labels:
      # Route to specific channel
      slack: kube-alerts-live
      severity: critical

# Provision dashboards to grafana
provisionDashboards:
  enabled: true
  dashboardLabel: grafana_dashboard
